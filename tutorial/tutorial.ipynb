{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cdc425-8074-4178-897f-cc4741365dd7",
   "metadata": {},
   "source": [
    "# Getting started with TabFormerLite\n",
    "\n",
    "In this tutorial, we will show how to use the library to accomplish any of the following tasks:\n",
    "\n",
    "- Pre-processing input data to be compatible with the model.\n",
    "- Pre-training the model through masked language modeling (MLM) task.\n",
    "- Extracting embeddings from pre-trained models using various pooling strategies. These embeddings can be used as input to simpler machine-learning models to perform downstream tasks.\n",
    "- Fine-tuning the model on a binary classification task.\n",
    "\n",
    "In this tutorial, we will use the transaction dataset, a synthetic corpus for credit card transactions that has been made available by the authors of the paper [Tabular Transformers for modeling multivariate time series](https://arxiv.org/abs/2011.01843). The card transaction dataset can be downloaded from this [link](https://ibm.ent.box.com/v/tabformer-data).\n",
    "\n",
    "## 0. Data loading and cleaning\n",
    "\n",
    "Let's start by loading the transaction dataset in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "729f73dd-fc35-49c5-891a-5d4df9a6839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f434d047-3309-4568-a480-b82e63a6eaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (16897222, 15)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Card</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Use Chip</th>\n",
       "      <th>Merchant Name</th>\n",
       "      <th>Merchant City</th>\n",
       "      <th>Merchant State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Errors?</th>\n",
       "      <th>Is Fraud?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06:21</td>\n",
       "      <td>$134.09</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>3.527213e+18</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06:42</td>\n",
       "      <td>$38.48</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-7.276121e+17</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>06:22</td>\n",
       "      <td>$120.34</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-7.276121e+17</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17:45</td>\n",
       "      <td>$128.95</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>3.414527e+18</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5651.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>06:23</td>\n",
       "      <td>$104.71</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>5.817218e+18</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5912.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Card  Year  Month  Day   Time   Amount           Use Chip  \\\n",
       "0     0     0  2002      9  1.0  06:21  $134.09  Swipe Transaction   \n",
       "1     0     0  2002      9  1.0  06:42   $38.48  Swipe Transaction   \n",
       "2     0     0  2002      9  2.0  06:22  $120.34  Swipe Transaction   \n",
       "3     0     0  2002      9  2.0  17:45  $128.95  Swipe Transaction   \n",
       "4     0     0  2002      9  3.0  06:23  $104.71  Swipe Transaction   \n",
       "\n",
       "   Merchant Name  Merchant City Merchant State      Zip     MCC Errors?  \\\n",
       "0   3.527213e+18       La Verne             CA  91750.0  5300.0     NaN   \n",
       "1  -7.276121e+17  Monterey Park             CA  91754.0  5411.0     NaN   \n",
       "2  -7.276121e+17  Monterey Park             CA  91754.0  5411.0     NaN   \n",
       "3   3.414527e+18  Monterey Park             CA  91754.0  5651.0     NaN   \n",
       "4   5.817218e+18       La Verne             CA  91750.0  5912.0     NaN   \n",
       "\n",
       "  Is Fraud?  \n",
       "0        No  \n",
       "1        No  \n",
       "2        No  \n",
       "3        No  \n",
       "4        No  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to dataset\n",
    "data_path = \"./data/card_transaction.v1.csv\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Data shape: {df.shape}\\n\")\n",
    "\n",
    "# Show a few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "708758c7-f5dc-4aaf-bd70-252cb66a7426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1375"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique users\n",
    "len(df[\"User\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ef4f43-88a4-4a56-960f-453c0fae157c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest year in data: 1991\n",
      "Latest year in data: 2020\n"
     ]
    }
   ],
   "source": [
    "# Show year span in data\n",
    "print(f\"Earliest year in data: {df['Year'].min()}\")\n",
    "print(f\"Latest year in data: {df['Year'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f02cd6-3334-48a0-b228-8e86eb1f9aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     16876654\n",
       "Yes       20567\n",
       "Name: Is Fraud?, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show class imbalance in target label\n",
    "df[\"Is Fraud?\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228956ff-e18d-4191-84cf-9c5c78bdb66b",
   "metadata": {},
   "source": [
    "The transactions dataset has approx. 16.9M transactions from 1,375 users spanning between 1991 and 2020. Each transaction has 12 columns of categorical, continuous, and discrete variables. The label column is: `Is Fraud?`, where 20,567 samples are labeled fraudulent.\n",
    "\n",
    "Before feeding our data through the script for encoding raw tabular data, we need to address a couple of issues in the data, such as missing values, duplicated values, and uninformative characters (e.g., the `$` in the `Amount` column).\n",
    "\n",
    "The cleaning steps implemented below were inspired by the work in this [repository](https://github.com/IBM/TabFormer/blob/main/README.md) and are specific to the card transactions dataset. You may have to adapt these steps to your particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c43cdcc-5cba-4ff4-853b-35d4244831ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape before dropping duplicates: (16897222, 15)\n",
      "Data shape after dropping duplicates: (16897180, 15)\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicated entries\n",
    "\n",
    "print(f\"Data shape before dropping duplicates: {df.shape}\")\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Data shape after dropping duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b38100e-8126-4006-80bc-e56d135b726e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Errors?           16626102\n",
       "Zip                2064608\n",
       "Merchant State     1952676\n",
       "Day                      1\n",
       "Time                     1\n",
       "Amount                   1\n",
       "Use Chip                 1\n",
       "Merchant Name            1\n",
       "Merchant City            1\n",
       "MCC                      1\n",
       "Is Fraud?                1\n",
       "User                     0\n",
       "Card                     0\n",
       "Year                     0\n",
       "Month                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data for missing values\n",
    "\n",
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cc8940d-84ff-48c8-9466-cf0817bd1f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (16897179, 15)\n"
     ]
    }
   ],
   "source": [
    "# We drop the row where the target label is missing\n",
    "\n",
    "df = df.loc[~df[\"Is Fraud?\"].isna()]\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b9f0c7-792d-4ab9-930b-2aabbabfa516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address missing values in the other columns\n",
    "\n",
    "df.loc[:, \"Errors?\"].fillna(value=\"None\", inplace=True)\n",
    "df.loc[:, \"Zip\"].fillna(value=0, inplace=True)\n",
    "df.loc[:, \"Merchant State\"].fillna(value=\"None\", inplace=True)\n",
    "df.loc[:, \"Use Chip\"].fillna(value=\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c89603f9-697b-4e5c-bf87-17a583efbaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "User              0\n",
       "Card              0\n",
       "Year              0\n",
       "Month             0\n",
       "Day               0\n",
       "Time              0\n",
       "Amount            0\n",
       "Use Chip          0\n",
       "Merchant Name     0\n",
       "Merchant City     0\n",
       "Merchant State    0\n",
       "Zip               0\n",
       "MCC               0\n",
       "Errors?           0\n",
       "Is Fraud?         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check again for missing values\n",
    "\n",
    "df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16266408-3127-4140-be93-0b2085c0b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply integer encoding to target column\n",
    "\n",
    "df[\"Is Fraud?\"] = df[\"Is Fraud?\"].apply(lambda x: x == \"Yes\").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7aef8b-280e-40c9-a133-1108447086f5",
   "metadata": {},
   "source": [
    "**The Amount column** \n",
    "\n",
    "Below, we apply the following cleaning steps to the column `Amount`:\n",
    "\n",
    "- remove the prefix \"$\"\n",
    "- convert to float data type\n",
    "- set to zero any negative values in the Amount column\n",
    "- apply log-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ecdbad-b0e8-45bb-a1f0-9094e5ae2513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    $134.09\n",
       "1     $38.48\n",
       "2    $120.34\n",
       "3    $128.95\n",
       "4    $104.71\n",
       "Name: Amount, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few samples from the \"Amount\" column\n",
    "\n",
    "df[\"Amount\"][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e87541d-c7dd-4093-abe6-5d1d686bd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove $ and convert to float\n",
    "df[\"Amount\"] = df[\"Amount\"].str.replace(\"$\", \"\", regex=True).astype(\"float\")\n",
    "\n",
    "# Set any negative amounts to zero\n",
    "df[\"Amount\"] = df[\"Amount\"].apply(lambda x: max(0, x))\n",
    "\n",
    "# Apply log-transformation\n",
    "df[\"Amount\"] = np.log1p(df[\"Amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a026a364-31da-4569-8d12-13f5925f1d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4.905941\n",
       "1    3.675794\n",
       "2    4.798597\n",
       "3    4.867150\n",
       "4    4.660699\n",
       "Name: Amount, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few samples from the \"Amount\" column after cleaning\n",
    "\n",
    "df[\"Amount\"][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c0b16-7f41-43b8-93da-a1168f12808e",
   "metadata": {},
   "source": [
    "**Datetime columns**\n",
    "\n",
    "Below, we combine all datetime columns (`Year`, `Month`, `Day` and `Time`) into a column that we will call \"TimeStamp\", representing the number of nanoseconds since the beginning of the Unix epoch on January 1, 1970."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ca7ac8e-e7f8-41d2-917e-ea7c2e9fd214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unify datetime columns in a single \"TimeStamp\" column\n",
    "\n",
    "\n",
    "def timeEncoder(X):\n",
    "    X_hm = X[\"Time\"].str.split(\":\", expand=True)\n",
    "    d = pd.to_datetime(\n",
    "        dict(\n",
    "            year=X[\"Year\"], month=X[\"Month\"], day=X[\"Day\"], hour=X_hm[0], minute=X_hm[1]\n",
    "        )\n",
    "    ).astype(int)\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "\n",
    "df[\"TimeStamp\"] = timeEncoder(df[[\"Year\", \"Month\", \"Day\", \"Time\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d1047-2934-4181-b1ad-32df1ba8b340",
   "metadata": {},
   "source": [
    "We will also combine all datetime columns (`Year`, `Month`, `Day`, and `Time`) into a column that we will call \"event_dt\" to represent the date and time of each transaction.\n",
    "\n",
    "Note that the columns `User` and `event_dt` will not be fed to the model. These columns will be used to sort the transactions by the user and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0003941-0082-4498-8166-ec887e5ca058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create event_dt column\n",
    "\n",
    "df[\"event_dt\"] = pd.to_datetime(\n",
    "    df[\"Year\"].astype(\"str\")\n",
    "    + \"-\"\n",
    "    + df[\"Month\"].astype(\"str\")\n",
    "    + \"-\"\n",
    "    + df[\"Day\"].astype(\"int\").astype(\"str\")\n",
    "    + \" \"\n",
    "    + df[\"Time\"],\n",
    "    format=\"%Y-%m-%d %H:%M\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76fb9bc2-c5d2-4f33-8b72-02c200314129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Card</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Time</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Use Chip</th>\n",
       "      <th>Merchant Name</th>\n",
       "      <th>Merchant City</th>\n",
       "      <th>Merchant State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Errors?</th>\n",
       "      <th>Is Fraud?</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>event_dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06:21</td>\n",
       "      <td>4.905941</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>3.527213e+18</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5300.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1030861260000000000</td>\n",
       "      <td>2002-09-01 06:21:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>06:42</td>\n",
       "      <td>3.675794</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-7.276121e+17</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1030862520000000000</td>\n",
       "      <td>2002-09-01 06:42:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>06:22</td>\n",
       "      <td>4.798597</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-7.276121e+17</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1030947720000000000</td>\n",
       "      <td>2002-09-02 06:22:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   User  Card  Year  Month  Day   Time    Amount           Use Chip  \\\n",
       "0     0     0  2002      9  1.0  06:21  4.905941  Swipe Transaction   \n",
       "1     0     0  2002      9  1.0  06:42  3.675794  Swipe Transaction   \n",
       "2     0     0  2002      9  2.0  06:22  4.798597  Swipe Transaction   \n",
       "\n",
       "   Merchant Name  Merchant City Merchant State      Zip     MCC Errors?  \\\n",
       "0   3.527213e+18       La Verne             CA  91750.0  5300.0    None   \n",
       "1  -7.276121e+17  Monterey Park             CA  91754.0  5411.0    None   \n",
       "2  -7.276121e+17  Monterey Park             CA  91754.0  5411.0    None   \n",
       "\n",
       "   Is Fraud?            TimeStamp            event_dt  \n",
       "0          0  1030861260000000000 2002-09-01 06:21:00  \n",
       "1          0  1030862520000000000 2002-09-01 06:42:00  \n",
       "2          0  1030947720000000000 2002-09-02 06:22:00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4928a-4a9c-4cdb-9b1f-7cd9b8d78b68",
   "metadata": {},
   "source": [
    "After creating the columns \"TimeStamp\" and \"event_dt\", the original datetime columns (`Year`, `Month`, `Day`, and `Time`) are not useful and can be removed. Below, we create a list of columns we want to keep for encoding the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "538da72f-a459-4169-b730-85306410e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"User\",\n",
    "    \"event_dt\",\n",
    "    \"Card\",\n",
    "    \"TimeStamp\",\n",
    "    \"Amount\",\n",
    "    \"Use Chip\",\n",
    "    \"Merchant Name\",\n",
    "    \"Merchant City\",\n",
    "    \"Merchant State\",\n",
    "    \"Zip\",\n",
    "    \"MCC\",\n",
    "    \"Errors?\",\n",
    "    \"Is Fraud?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39fa81-9f16-4eb0-8a85-31b08a85a0b4",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "So far, we addressed missing and duplicated values, cleaned the column 'Amount', and created two new columns (\"TimeStamp\" and \"event_dt\") from the DateTime features that were initially available in the data. \n",
    "\n",
    "We will now split the dataset into:\n",
    "- data to use to pre-train the model (we call this data: \"pretraining_data\")\n",
    "- data for extracting embeddings or fine-tuning the model (we call this data: \"inference_data\")\n",
    "\n",
    "To prevent \"data leakage\" between the data for pre-training the model and the data for inference, we diligently use all data before 2019 to pre-train the data and the data after 2019 for inference (the year choice is arbitrary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18b6f63b-7ef5-43bf-a838-5b5376de8cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training data shape: (15478667, 13)\n",
      "Inference data shape: (973021, 13)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "\n",
    "# Pre-training data: < 2019\n",
    "# Inference data: 2019 - 2020\n",
    "\n",
    "pretraining_data = df.loc[df[\"Year\"] < 2019, columns_to_keep]\n",
    "\n",
    "# df[\"event_dt\"] <= '2019-10-27': No pos_labels in inference data after this date\n",
    "inference_data = df.loc[\n",
    "    (df[\"Year\"] >= 2019) & (df[\"event_dt\"] <= \"2019-10-27\"), columns_to_keep\n",
    "]\n",
    "\n",
    "print(f\"Pre-training data shape: {pretraining_data.shape}\")\n",
    "print(f\"Inference data shape: {inference_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7bc6c2-8be2-4afb-8d40-10b7de9be1bc",
   "metadata": {},
   "source": [
    "We will use the inference data to extract embeddings from pre-trained models and then will use these embeddings as input to simpler machine learning models to perform downstream tasks.\n",
    "\n",
    "Therefore, we need to further split the inference data into train/validation/test sets to be ready for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "655b6392-4cdf-4704-a894-6ffc427a41ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (689732, 13)\n",
      "Validation data shape: (147094, 13)\n",
      "Test data shape: (136195, 13)\n"
     ]
    }
   ],
   "source": [
    "# Further split inference data into train/valid/test sets\n",
    "\n",
    "splitting_dates = {\"train_end_date\": \"2019-08-01\", \"valid_end_date\": \"2019-09-15\"}\n",
    "\n",
    "train_idx = inference_data[\"event_dt\"] < splitting_dates[\"train_end_date\"]\n",
    "valid_idx = (inference_data[\"event_dt\"] >= splitting_dates[\"train_end_date\"]) & (\n",
    "    inference_data[\"event_dt\"] < splitting_dates[\"valid_end_date\"]\n",
    ")\n",
    "test_idx = inference_data[\"event_dt\"] >= splitting_dates[\"valid_end_date\"]\n",
    "\n",
    "train_data = inference_data.loc[train_idx]\n",
    "validation_data = inference_data.loc[valid_idx]\n",
    "test_data = inference_data.loc[test_idx]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Validation data shape: {validation_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68515572-c95c-4c48-871b-3ffc80d2075b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    688770\n",
       "1       962\n",
       "Name: Is Fraud?, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking class imbalance in train_data\n",
    "train_data[\"Is Fraud?\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c07f155f-bb16-4043-bd8b-84f86888ceec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    146893\n",
       "1       201\n",
       "Name: Is Fraud?, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking class imbalance in validation_data\n",
    "validation_data[\"Is Fraud?\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b423673-6921-4015-8854-8000d9e60b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    135940\n",
       "1       255\n",
       "Name: Is Fraud?, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking class imbalance in test_data\n",
    "test_data[\"Is Fraud?\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc42646-7ad8-4ca4-9df3-176d2e59f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training/test data for easy reloading\n",
    "\n",
    "clean_data_dir = \"./data/card_dataset_clean\"\n",
    "# If unavailable, create directory\n",
    "os.makedirs(os.path.join(clean_data_dir, \"inference\"), mode=0o777, exist_ok=True)\n",
    "\n",
    "# Pretraining data\n",
    "pretraining_data.to_csv(\n",
    "    os.path.join(clean_data_dir, \"pretraining_data_clean.csv\"), index=False\n",
    ")\n",
    "\n",
    "# Inference data\n",
    "train_data.to_csv(\n",
    "    os.path.join(clean_data_dir, \"inference\", \"train_data_clean.csv\"), index=False\n",
    ")\n",
    "validation_data.to_csv(\n",
    "    os.path.join(clean_data_dir, \"inference\", \"valid_data_clean.csv\"), index=False\n",
    ")\n",
    "test_data.to_csv(\n",
    "    os.path.join(clean_data_dir, \"inference\", \"test_data_clean.csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f29a7-67fb-4d38-ac24-3d5b3f8e25be",
   "metadata": {},
   "source": [
    "## 1. Data pre-processing\n",
    "\n",
    "The model requires the input data to be, first, tokenized and encoded as unique numerical identifiers and then shaped into fixed-length sequences. The data pre-processing step prepares tabular data to be compatible with the model by implementing the following:\n",
    "\n",
    "Discretizes tabular data into discrete information units, much like NLP tokens, and assigns unique numerical identifiers to these units.\n",
    "A sliding window approach shapes the encoded tabular data into fixed-length sequences.\n",
    "\n",
    "To pre-process the training data, we use the following command:\n",
    "```\n",
    "$ python3 scripts/encode_dataset.py -cfg ./configs/example/data_encoding/config_card_dataset_encoding.json\n",
    "```\n",
    "\n",
    "Let's have a look at the configuration file `config_card_dataset_encoding.json`:\n",
    "\n",
    "* ```data_dir```: directory path where the csv file with the raw tabular data is available\n",
    "* ```data_name```: \"pretraining_data_clean.csv\"\n",
    "* ```user_col```: \"User\"\n",
    "* ```date_col```: \"event_dt\"\n",
    "* ```target_col```: \"Is Fraud?\"\n",
    "* ```seq_len```: 10\n",
    "* ```stride```: 5\n",
    "* ```num_bins```: 10\n",
    "* ```n_max```: 85000\n",
    "* ```encoded_data_dir```: directory path where the folder with the outputs of the pre-processing step will be saved\n",
    "* ```encoded_data_folder```: \"card_dataset_encoded_seq_len_10_stride_5_bins_10\"\n",
    "\n",
    "\n",
    "You can find an explanation of the parameters in the configuration file for pre-processing tabular data in the `README file`. Below, we show how we select the `n_max` value for the card transactions dataset. \n",
    "\n",
    "We start by counting the unique values in each column in the `pretraining_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "551bddb4-4ea1-44d3-b4d8-d14eec38ee3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>unique_value_counts</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Fraud?</td>\n",
       "      <td>2</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Use Chip</td>\n",
       "      <td>3</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Card</td>\n",
       "      <td>9</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Errors?</td>\n",
       "      <td>24</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MCC</td>\n",
       "      <td>109</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Merchant State</td>\n",
       "      <td>201</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>User</td>\n",
       "      <td>1106</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Merchant City</td>\n",
       "      <td>12724</td>\n",
       "      <td>object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Zip</td>\n",
       "      <td>25771</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Merchant Name</td>\n",
       "      <td>78388</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Amount</td>\n",
       "      <td>85645</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>event_dt</td>\n",
       "      <td>6413601</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TimeStamp</td>\n",
       "      <td>6413601</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       column_name  unique_value_counts       data_type\n",
       "0        Is Fraud?                    2           int64\n",
       "1         Use Chip                    3          object\n",
       "2             Card                    9           int64\n",
       "3          Errors?                   24          object\n",
       "4              MCC                  109         float64\n",
       "5   Merchant State                  201          object\n",
       "6             User                 1106           int64\n",
       "7    Merchant City                12724          object\n",
       "8              Zip                25771         float64\n",
       "9    Merchant Name                78388         float64\n",
       "10          Amount                85645         float64\n",
       "11        event_dt              6413601  datetime64[ns]\n",
       "12       TimeStamp              6413601           int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count unique values in each column\n",
    "\n",
    "unique_value_counts = []\n",
    "\n",
    "for col in columns_to_keep:\n",
    "    unique_value_counts.append(len(pretraining_data[col].unique()))\n",
    "\n",
    "# Collect values in a dataframe\n",
    "df_info = pd.DataFrame(\n",
    "    {\n",
    "        \"column_name\": columns_to_keep,\n",
    "        \"unique_value_counts\": unique_value_counts,\n",
    "        \"data_type\": pretraining_data[columns_to_keep].dtypes.values,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sort values by unique_value_counts and reset index\n",
    "df_info.sort_values(by=\"unique_value_counts\", ascending=True, inplace=True)\n",
    "df_info.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc06e2-5b70-4150-9bbf-a27174ea0aa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "The data pre-processing step will discretize any non-categorical column with more than `n_max` unique values using quantile binning. We use `n_max` = 85,000, meaning only the columns `Amount` and `TimeStamp` will be discretized using quantile binning. Note that the columns `User` and `event_dt` are used to sort and group rows in the data by the user and date and won't be fed to the model. Therefore, these columns will not be transformed during the pre-processing step.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "The pre-processing step generates and exports in the output directory all files that are necessary to pre-train the model:\n",
    "\n",
    "```\n",
    "└── encoded_data_dir/encoded_data_folder\n",
    "    ├── binning.pickle                 <- Pickle file with all encoders and bins used in quantile binning\n",
    "    ├── column_lists_by_dtype.json     <- Json file with lists of columns for each discretization strategy\n",
    "    ├── config.json                    <- configuration file used in pre-processing step\n",
    "    ├── processed_data_and_labels.h5   <- encoded data\n",
    "    ├── vocab_tokenizer.nb             <- Tokenizer\n",
    "    └── vocab.pickle                   <- Vocabulary\n",
    "```\n",
    "\n",
    "## 2. Pre-training\n",
    "\n",
    "The model is pre-trained using a \"masked language model\" (MLM) pre-training objective.\n",
    "\n",
    "Once the data pre-processing step is complete, we can pre-train the model using the following command.\n",
    "\n",
    "```\n",
    "$ python3 scripts/run_mlm_pretraining.py -cfg ./configs/example/pretraining/config_card_dataset_size_300.json\n",
    "```\n",
    "Let's have a look at the `config_card_dataset_size_300.json` configuration file:\n",
    "\n",
    "* \"seed\"                  <- random seed,\n",
    "* \"encoded_data_dir\"      <- directory path where the outputs of the preprocessing step are stored \n",
    "* \"encoded_data_folder\"   <- folder name containing the outputs of the preprocessing step\n",
    "* \"output_dir\"            <- directory path where the outputs of the pre-training step are stored \n",
    "* \"output_folder\"         <- folder name containing the outputs of the pre-training step\n",
    "* \"add_date_suffix_to_output\":  true,\n",
    "* \"field_hidden_size\": 256,\n",
    "* \"tab_embeddings_num_attention_heads\": 8,\n",
    "* \"tab_embedding_num_encoder_layers\": 2,\n",
    "* \"tab_embedding_dropout\": 0.1,\n",
    "* \"num_attention_heads\": 12,\n",
    "* \"num_hidden_layers\": 12,\n",
    "* \"hidden_size\": 300,\n",
    "* \"mlm_average_loss\": true,\n",
    "* \"mlm_probability\": 0.15,\n",
    "* \"batch_size\": 256,\n",
    "* \"grad_acc_steps\": 16,\n",
    "* \"logging_per_epoch\": 15,\n",
    "* \"num_epochs\": 20,\n",
    "* \"checkpoint_every_N_epochs\": 1,\n",
    "* \"lr_max\": 5e-4,\n",
    "* \"warmup_steps_in_epochs\": 1,\n",
    "* \"save_total_limit\": 5,\n",
    "* \"resume_from_checkpoint\": false\n",
    "* \"checkpoint_dir\": \"\"\n",
    "\n",
    "You can find an explanation of the parameters in the configuration file for pre-training in the `README file`.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "The pre-training step generates and exports several files in the output directory, for which we show one example below.\n",
    "\n",
    "```\n",
    "└── output_dir/output_folder\n",
    "    ├── checkpoint-...     <- checkpoint directory\n",
    "    ├── checkpoint-...     <- checkpoint directory\n",
    "    ├── logging            <- directory with logging files for TensorBoard  \n",
    "    ├── callback_log.json  <- dictionary with training/validation metrics\n",
    "    ├── config.json        <- configuration file used to launch pre-training\n",
    "    └── training_args.json <- dictionary with model configuration and training arguments used\n",
    "```\n",
    "\n",
    "### MLM metrics\n",
    "\n",
    "Below, we show an example of MLM metrics computed during the pre-training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6304c3-b714-480e-addf-7206d47917d1",
   "metadata": {},
   "source": [
    "![MLM_metrics](../docs/img/mlm_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a54ac-9f9c-4c21-a156-58c991c1ae2e",
   "metadata": {},
   "source": [
    "## 3. Embedding extraction from pre-trained models\n",
    "\n",
    "Once the pre-training phase is completed, we can extract embeddings for each time step within a time-series sequence using the following command:\n",
    "\n",
    "```\n",
    "$ python3 scripts/inference_main.py -cfg configs/example/inference/config_card_dataset_inference.json\n",
    "```\n",
    "\n",
    "Let's have a look at the `config_card_dataset_inference.json` configuration file:\n",
    "\n",
    "* \"seed\": <- random seed\n",
    "* \"pretrained_model_config\":\n",
    "    * \"model_directory\" <- directory path where the outputs of the pre-training step were stored (same as output_dir in the pre-training config file).\n",
    "    * \"model_name\" <- folder name inside model_directory containing one (or more) model checkpoints (same as output_folder in the pre-training config file).\n",
    "    * ckpt_dir\"<- This optional argument can be used to specify the desired checkpoint to use for loading a pre-trained model.\n",
    "* \"pretraining_data_config\":\n",
    "    * \"path_to_config_file\" <- path to the configuration file with information about the data used to pre-train the model. \n",
    "* \"inference_data_config\": \n",
    "    * \"data_directory\": <- the path to the directory containing the train/validation/test datasets we want to extract embeddings for. \n",
    "* \"inference_config\": \n",
    "    * \"batch_size\" <- batch size\n",
    "    * \"pooling_on_time_axis\": \n",
    "        * \"strategy\": \"mean_pooling\"\n",
    "        * \"nbr_days\": 3\n",
    "    * \"pooling_on_layer_axis\": \n",
    "        * \"strategy\": \"single_layer_pooling\",\n",
    "        * \"pooling_layer\": -1\n",
    "* \"downstream_task_config\":\n",
    "    * \"target_cols_to_include\": [\"Is Fraud?\"]\n",
    "    * \"path_to_data_with_labels\": <- the path to file(s) containing the user, date, and label columns that we want to merge with the extracted embeddings.\n",
    "    * \"output_dir\": the path to the output directory where the embeddings and labels will be saved.\n",
    "    * \"user_col\": \"User\",\n",
    "    * \"date_col\": \"event_dt\"\n",
    "    \n",
    "\n",
    "You can find a detailed explanation of the parameters in the configuration file for extracting embeddings in the `README file`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe51133-8364-46f6-b550-e4b9c5fd11d7",
   "metadata": {},
   "source": [
    "### Outputs\n",
    "\n",
    "The embedding extraction step extracts embeddings and exports them in parquet or npz files in the specified output directory. Below, we show one example for loading embeddings from parquet files.\n",
    "\n",
    "#### Example\n",
    "In the this example, we are pooling embeddings from the last hidden layer and applying mean pooling over 3 days on the time-axis.\n",
    "\n",
    "```\n",
    "\"pooling_on_time_axis\": {\n",
    "    \"strategy\": \"mean_pooling\",\n",
    "    \"nbr_days\": 3\n",
    "    },\n",
    "\"pooling_on_layer_axis\": {\n",
    "    \"strategy\": \"single_layer_pooling\",\n",
    "    \"pooling_layer\": -1\n",
    "    }\n",
    "```\n",
    "\n",
    "The embeddings were extracted in a parquet file and can be loaded in a dataframe, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c07f2099-d02a-41c0-8ad1-fbeaaf784f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (957301, 303)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>event_dt</th>\n",
       "      <th>Is Fraud?</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>...</th>\n",
       "      <th>f290</th>\n",
       "      <th>f291</th>\n",
       "      <th>f292</th>\n",
       "      <th>f293</th>\n",
       "      <th>f294</th>\n",
       "      <th>f295</th>\n",
       "      <th>f296</th>\n",
       "      <th>f297</th>\n",
       "      <th>f298</th>\n",
       "      <th>f299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-04 13:15:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.705230</td>\n",
       "      <td>-1.495667</td>\n",
       "      <td>-0.205145</td>\n",
       "      <td>-0.749355</td>\n",
       "      <td>1.149317</td>\n",
       "      <td>-1.173064</td>\n",
       "      <td>0.325741</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.700586</td>\n",
       "      <td>-0.535140</td>\n",
       "      <td>1.226801</td>\n",
       "      <td>-1.317099</td>\n",
       "      <td>-0.056494</td>\n",
       "      <td>0.276329</td>\n",
       "      <td>0.856759</td>\n",
       "      <td>1.554424</td>\n",
       "      <td>-0.468213</td>\n",
       "      <td>-0.706564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-04 13:16:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.759816</td>\n",
       "      <td>-1.421752</td>\n",
       "      <td>-0.251014</td>\n",
       "      <td>-0.832929</td>\n",
       "      <td>1.138421</td>\n",
       "      <td>-1.243008</td>\n",
       "      <td>0.310202</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.688424</td>\n",
       "      <td>-0.460548</td>\n",
       "      <td>1.270176</td>\n",
       "      <td>-1.363025</td>\n",
       "      <td>-0.075365</td>\n",
       "      <td>0.291442</td>\n",
       "      <td>0.885366</td>\n",
       "      <td>1.593894</td>\n",
       "      <td>-0.383234</td>\n",
       "      <td>-0.660581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-04 13:17:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.757202</td>\n",
       "      <td>-1.421100</td>\n",
       "      <td>-0.083118</td>\n",
       "      <td>-0.779717</td>\n",
       "      <td>1.104834</td>\n",
       "      <td>-1.295749</td>\n",
       "      <td>0.278178</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.590062</td>\n",
       "      <td>-0.463647</td>\n",
       "      <td>1.356076</td>\n",
       "      <td>-1.289437</td>\n",
       "      <td>-0.115599</td>\n",
       "      <td>0.237998</td>\n",
       "      <td>0.837648</td>\n",
       "      <td>1.718791</td>\n",
       "      <td>-0.319569</td>\n",
       "      <td>-0.651324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   User             event_dt  Is Fraud?        f0        f1        f2  \\\n",
       "0     0  2019-01-04 13:15:00          0 -1.705230 -1.495667 -0.205145   \n",
       "1     0  2019-01-04 13:16:00          0 -1.759816 -1.421752 -0.251014   \n",
       "2     0  2019-01-04 13:17:00          0 -1.757202 -1.421100 -0.083118   \n",
       "\n",
       "         f3        f4        f5        f6  ...      f290      f291      f292  \\\n",
       "0 -0.749355  1.149317 -1.173064  0.325741  ... -1.700586 -0.535140  1.226801   \n",
       "1 -0.832929  1.138421 -1.243008  0.310202  ... -1.688424 -0.460548  1.270176   \n",
       "2 -0.779717  1.104834 -1.295749  0.278178  ... -1.590062 -0.463647  1.356076   \n",
       "\n",
       "       f293      f294      f295      f296      f297      f298      f299  \n",
       "0 -1.317099 -0.056494  0.276329  0.856759  1.554424 -0.468213 -0.706564  \n",
       "1 -1.363025 -0.075365  0.291442  0.885366  1.593894 -0.383234 -0.660581  \n",
       "2 -1.289437 -0.115599  0.237998  0.837648  1.718791 -0.319569 -0.651324  \n",
       "\n",
       "[3 rows x 303 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = glob(\n",
    "    (\n",
    "        \"inference/\"\n",
    "        \"embeddings_with_labels/embedding_size_300/\"\n",
    "        \"checkpoint-**_last_hidden_single_layer_pooling_10_days_mean_pooling.parquet\"\n",
    "    )\n",
    ")[0]\n",
    "\n",
    "df_embeddings = pd.read_parquet(output_path)\n",
    "\n",
    "print(f\"Data shape: {df_embeddings.shape}\\n\")\n",
    "\n",
    "# Show a few samples\n",
    "df_embeddings.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5175b7d-53cc-481f-b562-82794b177f91",
   "metadata": {},
   "source": [
    "The dataframe `df_embeddings` provides the user ID number, the transaction date, the target column, and the extracted embeddings. These embeddings can be used as input with a simple machine learning model, such as XGBoost, to predict fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27febf-09bd-418c-a3fe-29f4766bb200",
   "metadata": {},
   "source": [
    "## 4. Fine-tuning\n",
    "\n",
    "To fine-tune the model we can use the following command:\n",
    "\n",
    "```\n",
    "$ python3 scripts/finetuning.py -cfg configs/example/finetuning/config_card_finetuning.json\n",
    "```\n",
    "\n",
    "Let's have a look at the `config_card_finetuning.json` configuration file:\n",
    "\n",
    "* \"pretrained_model_config\":\n",
    "    * \"model_directory\" <- directory path where the outputs of the pre-training step were stored (same as output_dir in the pre-training config file).\n",
    "    * \"model_name\" <- folder name inside model_directory containing one (or more) model checkpoints (same as output_folder in the pre-training config file).\n",
    "    * ckpt_dir\"<- This optional argument can be used to specify the desired checkpoint to use for loading a pre-trained model.\n",
    "* \"pretraining_data_config\":\n",
    "    * \"path_to_config_file\" <- path to the configuration file with information about the data used to pre-train the model. \n",
    "* \"finetuning_data_config\": \n",
    "    * \"data_directory\": <- the path to the directory containing the train/validation/test datasets we want to use for finetuning\n",
    "* \"training_config\": \n",
    "    * \"seed\" <- random seed\n",
    "    * \"batch_size\" <- batch size\n",
    "    * \"output_dir\": <- directory path where the outputs of the fine-tuning step are stored \n",
    "    * \"output_folder\": <- folder name containing the outputs of the fine-tuning step\n",
    "    * \"add_date_suffix_to_output\": if true, a date suffix is added at the end of the output_folder name\n",
    "    * \"batch_size\": batch_size\n",
    "    * \"grad_acc_steps\": 16\n",
    "    * \"logs_per_epoch\": 50,\n",
    "    * \"num_epochs\": 15,\n",
    "    * \"checkpoint_every_N_epochs\": 1,\n",
    "    * \"lr_max\": 1e-4,\n",
    "    * \"warmup_steps_in_epochs\": 2,\n",
    "    * \"save_tot_lim\": 1,\n",
    "    * \"problem_type\": \"classification\",\n",
    "    * \"compute_pos_weight\": true,\n",
    "    * \"pos_weight\": 716,\n",
    "    * \"load_weights_from_pretraining\": true,\n",
    "    * \"pos_label\": 1,\n",
    "    * \"metric_for_best_model\": \"f1_score_minority\"\n",
    "  }\n",
    "}    \n",
    "    \n",
    "You can find a detailed explanation of the parameters in the configuration file for fine-tuning the model in the `README file`.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Below, we show an example of the metrics computed on the validation data during fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c198dc4-5b1e-4824-88ea-d7326699cff8",
   "metadata": {},
   "source": [
    "![Finetuning_metrics](../docs/img/finetuning_metrics.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabformer_upgrade",
   "language": "python",
   "name": "tabformer_upgrade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
